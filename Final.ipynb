{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":150545,"sourceType":"datasetVersion","datasetId":70909}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport xgboost as xgb\nimport os\nimport mahotas\nfrom skimage.feature import hog, local_binary_pattern\nfrom skimage import exposure\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ndef compute_hog_features(image):\n    fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualize=True)\n    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n    return fd\n\ndef compute_lbp_features(image):\n    lbp = local_binary_pattern(image, P=8, R=1, method='uniform')\n    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n    return hist\n\ndef compute_texture_features(image):\n    textures = mahotas.features.haralick(image)\n    mean_texture = textures.mean(axis=0)\n    return mean_texture\n\ndef compute_moments_features(image):\n    moments = cv2.HuMoments(cv2.moments(image)).flatten()\n    return moments\n\ndef preprocess_image(image_path, image_size=(64, 64)):\n    original_image = cv2.imread(image_path)\n    original_image = cv2.resize(original_image, image_size)\n    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n    return original_image, gray_image\n\n# Load the dataset\nimage_count = {}\ndata = []\n\nfor root, _, files in os.walk('/kaggle/input/plantdisease/PlantVillage'):\n    disease = os.path.basename(root)\n    image_count[disease] = 0\n    \n    # Include images based on the starting word of the disease\n    if disease.lower().startswith(('tomato')):\n        print(disease)\n        for file in files:\n            if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.JPG') or file.endswith('.PNG') or file.endswith('JPEG') or file.endswith('jpeg'):\n                image_path = os.path.join(root, file)\n\n                if image_count[disease] >= 1000:\n                    continue\n\n                original_image, gray_image = preprocess_image(image_path)\n\n                # Compute features\n                hog_features = compute_hog_features(gray_image)\n                lbp_features = compute_lbp_features(gray_image)\n                texture_features = compute_texture_features(gray_image)\n                moments_features = compute_moments_features(gray_image)\n\n                # Flatten the features\n                flattened_features = np.concatenate([\n                    original_image.flatten().astype(int),\n                    gray_image.flatten().astype(int),\n                    hog_features,\n                    lbp_features,\n                    texture_features,\n                    moments_features\n                ])\n\n                data.append([flattened_features, disease])\n                image_count[disease] += 1\n\ndf = pd.DataFrame(data, columns=['image_features', 'disease'])\n\n# Split the data into training and testing sets\nX = np.vstack(df['image_features'].to_numpy())\ny = df['disease']\n\n# Encode disease labels using LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n# Normalize or scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n# Create DMatrix for training and validation\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_test, label=y_test)\n\nparams = {\n    'objective': 'multi:softmax',\n    'num_class': 10,  \n    'max_depth': 8,\n    'learning_rate': 0.05,\n    'eval_metric': 'mlogloss',  # Change to 'merror' if you want classification error instead of logloss\n    'n_estimators': 750,\n    'min_child_weight': 1,\n    'subsample': 0.6,\n    'colsample_bytree': 1.0,\n    'gamma': 0.5,\n    'reg_alpha': 0.1,\n    'reg_lambda': 2\n}\n\nnum_boost_round = 3000\n\nevals = [(dvalid, 'eval')]\nearly_stopping_rounds = 10\n\n# Train the model\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=evals,\n    early_stopping_rounds=early_stopping_rounds\n)\n\n# Make predictions on the validation set\ny_pred = model.predict(dvalid)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-12-14T12:52:45.485204Z","iopub.execute_input":"2023-12-14T12:52:45.486030Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Tomato_Leaf_Mold\nTomato__Tomato_YellowLeaf__Curl_Virus\nTomato_Bacterial_spot\nTomato_Septoria_leaf_spot\nTomato_healthy\nTomato_Spider_mites_Two_spotted_spider_mite\nTomato_Early_blight\nTomato__Target_Spot\nTomato_Late_blight\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = RandomForestClassifier(n_estimators=100,random_state=42)\n# model.fit(X_train, y_train)\n\n# # Make predictions\n# y_pred = model.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Define the parameter grid for Grid Search\n# param_grid = {\n#     'n_estimators': [50, 100, 200],\n#     'max_depth': [None, 10, 20],\n#     'min_samples_split': [2, 5, 10],\n#     'min_samples_leaf': [1, 2, 4]\n# }\n\n# # Create a RandomForestClassifier\n# rf = RandomForestClassifier(random_state=42)\n\n# # Initialize GridSearchCV\n# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train, y_train)\n\n# # Print the best parameters found by Grid Search\n# print(\"Best Parameters:\", grid_search.best_params_)\n\n# # Get the best model\n# best_rf = grid_search.best_estimator_\n# y_pred = best_rf.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_dist = {\n#     'max_depth': [3, 5, 7, 10, None],\n#     'learning_rate': [0.01, 0.1, 0.2, 0.3],\n#     'n_estimators': [50, 100, 200, 300],\n#     'subsample': [0.8, 0.9, 1.0],\n#     'colsample_bytree': [0.8, 0.9, 1.0],\n#     'gamma': [0, 1, 2],\n#     'min_child_weight': [1, 2, 3]\n# }\n\n# # Create an XGBClassifier\n# xgb_model = xgb.XGBClassifier(random_state=42)\n\n# # Initialize RandomizedSearchCV\n# random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3, n_jobs=-1, random_state=42)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters found by RandomizedSearchCV\n# print(\"Best Parameters:\", random_search.best_params_)\n\n# # Get the best model\n# best_xgb = random_search.best_estimator_\n\n# # Make predictions on the test set\n# y_pred = best_xgb.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import xgboost as xgb\n# # Now you can use XGBoost with the binary labels\n# model = xgb.XGBClassifier()\n# model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = model.predict(X_test)\n\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train[0])","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(image_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_pred, average='weighted')\nf1","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}