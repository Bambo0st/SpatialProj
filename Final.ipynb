{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":150545,"sourceType":"datasetVersion","datasetId":70909}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install scikit-image\n# !pip install --upgrade scipy scikit-image\n\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport xgboost as xgb\nimport os\nimport mahotas\n# from skimage import exposure\nfrom skimage.feature import hog, local_binary_pattern\n\n\nimage_count = {}\ndefault_image_size = tuple((64, 64))  # Reduced image size\n\ndata = []\ndef compute_hog_features(image):\n    fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualize=True)\n    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n    return fd\n\n# Function to compute LBP features\ndef compute_lbp_features(image):\n    lbp = local_binary_pattern(image, P=8, R=1, method='uniform')\n    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n    return hist\n\nfor root, _, files in os.walk('/kaggle/input/plantdisease/PlantVillage'):\n    disease = os.path.basename(root)\n    image_count[disease] = 0\n    \n    # Include images based on the starting word of the disease\n    if disease.lower().startswith(('tomato')):\n        print(disease)\n        for file in files:\n            if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.JPG') or file.endswith('.PNG') or file.endswith('JPEG') or file.endswith('jpeg'):\n                image_path = os.path.join(root, file)\n\n                if image_count[disease] >= 500:\n                    continue\n\n                # Read the original image and resize\n                original_image = cv2.imread(image_path)\n                original_image = cv2.resize(original_image, default_image_size)\n\n                # Perform Canny edge detection\n                gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n                edges = cv2.Canny(gray_image, 50, 150)\n\n                # Compute color histograms for each channel\n                hist_b = cv2.calcHist([original_image], [0], None, [256], [0, 256]).flatten().astype(int)\n                hist_g = cv2.calcHist([original_image], [1], None, [256], [0, 256]).flatten().astype(int)\n                hist_r = cv2.calcHist([original_image], [2], None, [256], [0, 256]).flatten().astype(int)\n\n                # Compute GLCM texture features\n                textures = mahotas.features.haralick(gray_image)\n                mean_texture = textures.mean(axis=0)\n                \n                # Compute shape features using Hu Moments\n                moments = cv2.HuMoments(cv2.moments(gray_image)).flatten()\n\n                hog_features = compute_hog_features(gray_image)\n\n                lbp_features = compute_lbp_features(gray_image)\n\n                # Flatten the images and histograms to 1D arrays and convert to NumPy array\n                flattened_original_image = original_image.flatten().astype(int)\n                flattened_edges = edges.flatten().astype(int)\n                flattened_hist_b = hist_b.astype(int)\n                flattened_hist_g = hist_g.astype(int)\n                flattened_hist_r = hist_r.astype(int)\n                flattened_texture = mean_texture.astype(int)\n                flattened_moments = moments.astype(int)\n\n                # Concatenate the flattened pixel values, histograms, texture, moments, and disease name in data\n                combined_features = np.concatenate([\n                    flattened_original_image, \n                    flattened_edges, \n                    flattened_hist_b, \n                    flattened_hist_g, \n                    flattened_hist_r, \n                    flattened_texture, \n                    flattened_moments,\n                    hog_features,\n                    lbp_features\n                ])                \n                data.append([combined_features, disease])\n\n                # Increment the counter for the current disease\n                image_count[disease] += 1\n\n\ndf = pd.DataFrame(data, columns=['image_pixels', 'disease'])\n\n# Split the data into training and testing sets\nX = np.vstack(df['image_pixels'].to_numpy())\ny = df['disease']\n\n# Encode disease labels using LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model (using RandomForest as an example)\n# model = RandomForestClassifier(random_state=42)\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-12-14T11:42:56.601643Z","iopub.execute_input":"2023-12-14T11:42:56.602563Z","iopub.status.idle":"2023-12-14T12:00:05.765186Z","shell.execute_reply.started":"2023-12-14T11:42:56.602521Z","shell.execute_reply":"2023-12-14T12:00:05.763983Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.3)\nCollecting scipy\n  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/e0/9e/80e2205d138960a49caea391f3710600895dd8292b6868dc9aff7aa593f9/scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.21.0)\nCollecting scikit-image\n  Obtaining dependency information for scikit-image from https://files.pythonhosted.org/packages/f1/6c/49f5a0ce8ddcdbdac5ac69c129654938cc6de0a936303caa6cad495ceb2a/scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.24.3)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (10.1.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.8.12)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.0.9)\nDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, scikit-image\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.3\n    Uninstalling scipy-1.11.3:\n      Successfully uninstalled scipy-1.11.3\n  Attempting uninstall: scikit-image\n    Found existing installation: scikit-image 0.21.0\n    Uninstalling scikit-image-0.21.0:\n      Successfully uninstalled scikit-image-0.21.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-image-0.22.0 scipy-1.11.4\nTomato_Leaf_Mold\nTomato__Tomato_YellowLeaf__Curl_Virus\nTomato_Bacterial_spot\nTomato_Septoria_leaf_spot\nTomato_healthy\nTomato_Spider_mites_Two_spotted_spider_mite\nTomato_Early_blight\nTomato__Target_Spot\nTomato_Late_blight\nTomato__Tomato_mosaic_virus\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.84\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.91      0.89       107\n           1       0.67      0.82      0.74        93\n           2       0.82      0.64      0.72        96\n           3       0.85      0.84      0.85       112\n           4       0.79      0.73      0.76       101\n           5       0.86      0.91      0.89        93\n           6       0.86      0.87      0.86        95\n           7       0.91      0.87      0.89       102\n           8       0.88      0.86      0.87        84\n           9       0.92      0.97      0.94        92\n\n    accuracy                           0.84       975\n   macro avg       0.84      0.84      0.84       975\nweighted avg       0.84      0.84      0.84       975\n\nConfusion Matrix:\n [[97  1  4  1  2  0  0  1  0  1]\n [ 4 76  4  3  2  1  0  3  0  0]\n [ 1 19 61  2  4  0  4  3  0  2]\n [ 1  8  0 94  4  3  0  0  2  0]\n [ 4  4  3  6 74  0  3  1  6  0]\n [ 0  0  0  2  1 85  4  1  0  0]\n [ 1  1  0  0  1  6 83  0  0  3]\n [ 3  4  1  0  1  2  1 89  1  0]\n [ 0  0  0  2  5  1  2  0 72  2]\n [ 0  0  1  0  0  1  0  0  1 89]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = RandomForestClassifier(n_estimators=100,random_state=42)\n# model.fit(X_train, y_train)\n\n# # Make predictions\n# y_pred = model.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:41:09.206758Z","iopub.status.idle":"2023-12-14T11:41:09.207708Z","shell.execute_reply.started":"2023-12-14T11:41:09.207412Z","shell.execute_reply":"2023-12-14T11:41:09.207441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Define the parameter grid for Grid Search\n# param_grid = {\n#     'n_estimators': [50, 100, 200],\n#     'max_depth': [None, 10, 20],\n#     'min_samples_split': [2, 5, 10],\n#     'min_samples_leaf': [1, 2, 4]\n# }\n\n# # Create a RandomForestClassifier\n# rf = RandomForestClassifier(random_state=42)\n\n# # Initialize GridSearchCV\n# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train, y_train)\n\n# # Print the best parameters found by Grid Search\n# print(\"Best Parameters:\", grid_search.best_params_)\n\n# # Get the best model\n# best_rf = grid_search.best_estimator_\n# y_pred = best_rf.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:41:09.209292Z","iopub.status.idle":"2023-12-14T11:41:09.209827Z","shell.execute_reply.started":"2023-12-14T11:41:09.209553Z","shell.execute_reply":"2023-12-14T11:41:09.209579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_dist = {\n#     'max_depth': [3, 5, 7, 10, None],\n#     'learning_rate': [0.01, 0.1, 0.2, 0.3],\n#     'n_estimators': [50, 100, 200, 300],\n#     'subsample': [0.8, 0.9, 1.0],\n#     'colsample_bytree': [0.8, 0.9, 1.0],\n#     'gamma': [0, 1, 2],\n#     'min_child_weight': [1, 2, 3]\n# }\n\n# # Create an XGBClassifier\n# xgb_model = xgb.XGBClassifier(random_state=42)\n\n# # Initialize RandomizedSearchCV\n# random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3, n_jobs=-1, random_state=42)\n\n# # Fit the random search to the data\n# random_search.fit(X_train, y_train)\n\n# # Print the best parameters found by RandomizedSearchCV\n# print(\"Best Parameters:\", random_search.best_params_)\n\n# # Get the best model\n# best_xgb = random_search.best_estimator_\n\n# # Make predictions on the test set\n# y_pred = best_xgb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:41:09.212123Z","iopub.status.idle":"2023-12-14T11:41:09.212890Z","shell.execute_reply.started":"2023-12-14T11:41:09.212507Z","shell.execute_reply":"2023-12-14T11:41:09.212532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import xgboost as xgb\n# # Now you can use XGBoost with the binary labels\n# model = xgb.XGBClassifier()\n# model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:41:09.214699Z","iopub.status.idle":"2023-12-14T11:41:09.216861Z","shell.execute_reply.started":"2023-12-14T11:41:09.216327Z","shell.execute_reply":"2023-12-14T11:41:09.216381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = model.predict(X_test)\n\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-12-14T11:41:09.219219Z","iopub.status.idle":"2023-12-14T11:41:09.219734Z","shell.execute_reply.started":"2023-12-14T11:41:09.219476Z","shell.execute_reply":"2023-12-14T11:41:09.219505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train[0])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-12-14T11:41:09.221546Z","iopub.status.idle":"2023-12-14T11:41:09.222802Z","shell.execute_reply.started":"2023-12-14T11:41:09.222484Z","shell.execute_reply":"2023-12-14T11:41:09.222514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(image_count)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:41:09.224352Z","iopub.status.idle":"2023-12-14T11:41:09.225116Z","shell.execute_reply.started":"2023-12-14T11:41:09.224827Z","shell.execute_reply":"2023-12-14T11:41:09.224851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:02:37.521865Z","iopub.execute_input":"2023-12-14T12:02:37.522709Z","iopub.status.idle":"2023-12-14T12:02:37.530326Z","shell.execute_reply.started":"2023-12-14T12:02:37.522668Z","shell.execute_reply":"2023-12-14T12:02:37.529039Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Accuracy: 0.841025641025641\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_pred, average='weighted')\nf1","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-12-14T12:02:40.081157Z","iopub.execute_input":"2023-12-14T12:02:40.081571Z","iopub.status.idle":"2023-12-14T12:02:40.093518Z","shell.execute_reply.started":"2023-12-14T12:02:40.081540Z","shell.execute_reply":"2023-12-14T12:02:40.092522Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.8400113401554286"},"metadata":{}}]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-12-14T11:41:09.230794Z","iopub.status.idle":"2023-12-14T11:41:09.231626Z","shell.execute_reply.started":"2023-12-14T11:41:09.231336Z","shell.execute_reply":"2023-12-14T11:41:09.231365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:02:45.046909Z","iopub.execute_input":"2023-12-14T12:02:45.047323Z","iopub.status.idle":"2023-12-14T12:02:45.068507Z","shell.execute_reply.started":"2023-12-14T12:02:45.047293Z","shell.execute_reply":"2023-12-14T12:02:45.067447Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Accuracy: 0.84\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.91      0.89       107\n           1       0.67      0.82      0.74        93\n           2       0.82      0.64      0.72        96\n           3       0.85      0.84      0.85       112\n           4       0.79      0.73      0.76       101\n           5       0.86      0.91      0.89        93\n           6       0.86      0.87      0.86        95\n           7       0.91      0.87      0.89       102\n           8       0.88      0.86      0.87        84\n           9       0.92      0.97      0.94        92\n\n    accuracy                           0.84       975\n   macro avg       0.84      0.84      0.84       975\nweighted avg       0.84      0.84      0.84       975\n\nConfusion Matrix:\n [[97  1  4  1  2  0  0  1  0  1]\n [ 4 76  4  3  2  1  0  3  0  0]\n [ 1 19 61  2  4  0  4  3  0  2]\n [ 1  8  0 94  4  3  0  0  2  0]\n [ 4  4  3  6 74  0  3  1  6  0]\n [ 0  0  0  2  1 85  4  1  0  0]\n [ 1  1  0  0  1  6 83  0  0  3]\n [ 3  4  1  0  1  2  1 89  1  0]\n [ 0  0  0  2  5  1  2  0 72  2]\n [ 0  0  1  0  0  1  0  0  1 89]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}